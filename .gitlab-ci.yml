image: humancellatlas/dss-build-box
# The Docker image `humancellatlas/dss-build-box` is created through a manual process from
# `${DSS_HOME}/Dockerfile.allspark`. See the contents of `${DSS_HOME}/Dockerfile.allspark`
# creation and usage instructions.

stages:
  - test
  - deploy
  - delete

before_script:
  - echo "branch name - ${CI_COMMIT_REF_NAME}"
  - virtualenv ~/venv
  - source ~/venv/bin/activate
  - pip install -r requirements.txt
  - source environment
  - if [[ -f "environment.$CI_COMMIT_REF_NAME" ]]; then
  -   source environment.$CI_COMMIT_REF_NAME
  - fi
  - scripts/fetch_secret.sh gcp-credentials.json > gcp-credentials.json
  - export GOOGLE_APPLICATION_CREDENTIALS=$(pwd -P)/gcp-credentials.json
  - data_store_env=$(aws ssm get-parameter --name /dcp/dss/${DDS_DEPLOYMENT_STAGE}/environment | jq .Parameter.Value)
  - data_store_env_json=$(echo $data_store_env | python -c "import sys, json; print(json.load(sys.stdin))")
  - export DSS_S3_BUCKET=$(echo $data_store_env_json | jq -r .DSS_S3_BUCKET)
  - export DSS_GS_BUCKET=$(echo $data_store_env_json | jq -r .DSS_GS_BUCKET)
  - export DSS_ES_ENDPOINT=$(echo $data_store_env_json | jq -r .DSS_ES_ENDPOINT)

test:
  stage: test
  script:
    - make test
  except:
    - tags
    - schedules

deploy:
  stage: deploy
  script:
    - make plan-infra
    - make deploy
  environment:
    name: $DDS_ENV
  only:
    - master
    - integration
    - staging
  except:
    - schedules

inclusion-protected-delete:
  stage: delete
  script:
    - scripts/queue_delete_all_items.py $DDS_DELETE_WITH_REPLICA
  only:
    refs:
      - schedules
    variables:
      - $DDS_DELETE_WITH_REPLICA

verify-inclusion-list:
  stage: delete
  script:
    - scripts/verify_inclusion_list.py
  only:
    refs:
      - schedules
    variables:
      - $DDS_VERIFY_INCLUSION_LIST
